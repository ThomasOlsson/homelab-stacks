version: "3.9"
networks:
  ai: {}

services:
  ollama:
    container_name: ollama
    image: ollama/ollama:latest
    environment:
      OLLAMA_NUM_CTX: "4096"
      OLLAMA_KEEP_ALIVE: "30m"
    volumes:
      - /srv/docker/ollama/data:/root/.ollama
    ports:
      - "11434:11434"
    networks: [ai]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:11434/api/tags"]
      interval: 15s
      timeout: 3s
      retries: 10
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  qdrant:
    container_name: qdrant
    image: qdrant/qdrant:latest
    volumes:
      - /srv/docker/qdrant/storage:/qdrant/storage
    ports:
      - "6333:6333"
    networks: [ai]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:6333/readyz"]
      interval: 15s
      timeout: 3s
      retries: 20

  vllm:
    container_name: vllm
    image: vllm/vllm-openai:latest
    profiles: ["vllm"]
    command: ["--model","meta-llama/Meta-Llama-3-8B-Instruct","--gpu-memory-utilization","0.40"]
    ports:
      - "8000:8000"
    networks: [ai]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8000/v1/models"]
      interval: 20s
      timeout: 4s
      retries: 20
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

